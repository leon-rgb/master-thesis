% !TeX spellcheck = en_US
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    %backgroundcolor=\color{gray!10},
    frame=lr,
    %captionpos=b,
    tabsize=2,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    keywordstyle=\bfseries,
    commentstyle=\itshape\color{gray},
    stringstyle=\ttfamily\color{darkgray},
    lineskip=0.1em  % Add space between lines
}

\chapter{Evaluation}
\label{chap:evaluation}

In this chapter, we present the evaluation methodology and results for the smart home chatbot. The evaluation focuses on two key aspects: the semantic similarity of the responses and the accuracy of the generated JSON commands. Additionally, we discuss the initial approach using classification metrics, the challenges encountered, and the refined approach to address these challenges.

\section{Study Design}
In this section we want to provide the base study design we came up.
The design of our evaluation was gathered through clearly defining the goals of the evaluation and coming to measurable metrics in the end via the top-down \gls{gqm} approach.
Our approach consists of three main goals: assessing the accuracy, the user experience and the explainability of the developed smart home chatbot.
Based on this we developed the whole evaluation process which consists of a \gls{llm} evaluation approach for the accuracy and a user study for the other two goals.
Details are provided later within this chapter.

\subsection{Goal Question Metric Paradigm}
The \gls{gqm} paradigm according to \citet{caldiera1994goal} provides a structured approach to evaluate different works in the area of Software Engineering and therefore is also suitable for evaluating various aspects of the smart home chatbot. 
Our evaluation framework consists of three primary goals, each addressing a specific area of interest: accuracy, user experience, and explainability.
This framework is shown in \cref{fig:gqm}

\textbf{Goal 1: Assess the Accuracy of the Smart Home Chatbot}

The first goal focuses on determining how accurately the chatbot can understand and respond to user commands. To achieve this, several questions are formulated:

\begin{itemize}
    \item \textbf{Q1: How accurate are the natural language answers of the language model?}
    \item \textbf{Q2: How accurate are the JSON responses of the language model?}
\end{itemize}

To answer these questions, relevant metrics are identified. Semantic similarity measures are used to evaluate the natural language responses, potentially incorporating other related metrics to ensure comprehensive assessment. JSON accuracy metrics are employed to evaluate the precision of the chatbot's structured responses. A combined metric of semantic similarity and JSON accuracy provides a holistic view of the chatbot's overall accuracy.

\textbf{Goal 2: Evaluate the User Experience of the Smart Home Chatbot}

The second goal is to understand the users' interaction experience with the chatbot. This involves evaluating how intuitive and satisfactory the chatbot is in performing tasks. The questions under this goal include:

\begin{itemize}
    \item \textbf{Q1: Are typical tasks easy to achieve?}
    \item \textbf{Q2: How satisfied are users with the chatbot's performance?}
    \item \textbf{Q3: What could be improved?}
    \item \textbf{Q4: Does the chatbot add to existing functionality of typical smart home applications?}
\end{itemize}

The metrics for these questions involve measuring task completion time, the number of attempts, and the success rate of task completion. User satisfaction is gauged through questionnaires administered after the experiment. These questionnaires assess various aspects of the user experience, including ease of use, overall satisfaction, and areas for improvement.

\textbf{Goal 3: Assess the Explainability of the Smart Home Chatbot}

The third goal addresses how well the chatbot can explain its actions and decisions to users, which is crucial for building trust and usability. The questions related to this goal are:

\begin{itemize}
    \item \textbf{Q1: How clear and understandable are the chatbot's explanations?}
    \item \textbf{Q2: What could be improved?}
\end{itemize}

To measure the explainability, semi-structured interviews are conducted after the experiment. These interviews delve into the clarity, transparency, and usefulness of the explanations provided by the chatbot, allowing for detailed qualitative feedback from users.

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.98\textwidth]{graphics/gqm.png}
    \caption{Visualized Goal Question Metric}
    \label{fig:gqm}
\end{figure}
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.75\textwidth]{graphics/eval-process.png}
    \caption{The whole evaluation process visualized}
    \label{fig:evalprocess}
\end{figure}

\subsection{Resulting Evaluation Process}
A Visualization of our evaluation process can be seen in \cref{fig:evalprocess}
Based on the obtained \gls{gqm}, the evaluation can be split into two parts: evaluating the model performance and a user study for examining User Experience and Explainability.
Besides the developed protoype chatbot the obtained sample user inputs can be greatly used in the evaluation process.
They could be used to construct the dataset that was essential for the evaluation of the language model.
This evaluation dataset contains for each sample input an expected natural language output and an eventually expected \gls{json} to measure both the accuracy of the output the user sees and the constructed \gls{json} that is used for further actions in the smart home system.

The other part is the user study in which users have a setup of devices that are supported by our prototype and receive a list of tasks in which the success should be measured and afterwards a questionnaire and a semi-structured interview are used to answer the questions regarding Goal 2 and 3 in the defined \gls{gqm}

Based on these two parts, the takeaway of this thesis can be received and constructed.
The results emerge directly as an output from the model evaluation and the user study when combined with quantitative and qualitative methods.

Based on this and the detailed evaluation setup the results can be discussed and threats to validity be debated.


\section{Model Performance}
\label{sec:modelperform}


\subsection{Hardware and Language Models used}
\label{subsec:hardware-llm}
The hardware setup was the same as described in \cref{subsec:modelcust} since it is very fast for the model sizes we wanted to test as also explained in the same section.
For measuring the model performance there was no need to have a setup of smart home devices since when the outputed \gls{json} of the model is correct the correct action will be triggered in the underlying system.
The language models we selected for evaluation are also described in \cref{subsec:modelcust}.
We pre-filtered these models based on three selected typical user interactions from the evaluation dataset we created. If a model was not able to answer with a suitable \gls{json} or did not even understand the structure of the message history provided we excluded it from further evaluation with the whole dataset since it would not be worth investing the resources for it.
Only the llama3 model has passed this test (with two version: the normal 8b parameters model and the instruct version of it) and is therefore included in the further evaluation. The other models were not able to answer based on the device list provided with each user request. They usually answered with phrases like ``I can't find a device named 'TV'.''

\subsection{Evaluation Dataset}
To assess the performance and capabilities of our smart home chatbot, we developed a comprehensive evaluation dataset with a total of 80 entries. This dataset is designed to simulate realistic user interactions and test the chatbot's ability to understand context, control devices, and provide informative responses.
The evaluation dataset consists of a series of input-output pairs, where each input represents a chat history and the output represents the expected response from the chatbot. 
In creating the inputs for our evaluation dataset, we leveraged the user inputs collected during our earlier survey (as described in Section 4.4). These real-world examples provided authentic phrasing and diverse query formulations typical of smart home users. We adapted and expanded upon these collected inputs to ensure they aligned with our specific test scenarios and device setups. This approach allowed us to create a more realistic and challenging evaluation dataset, closely mimicking the variety of natural language inputs a smart home chatbot would encounter in practical use.
The structure of each entry in the dataset is as follows:

\begin{enumerate}
    \item Input: A chat history containing a minimum of two messages from the user. The first message always includes a device list that provides crucial context about the user's smart home environment. For details on the device list structure, refer to \cref{sec:req-building}.
    \item Expected Output: A natural language response that the chatbot is expected to generate based on the given chat history.
    \item Expected JSON: A JSON object representing the action the chatbot should take, if any. The JSON includes only the necessary keys for each action:
    \begin{itemize}
    \item For 'turn-on' or 'turn-off' actions: 'action' and 'deviceID'
    \item For 'change-temperature' action: 'action', 'deviceID', and 'value'
    \item If no action is necessary, the expected JSON is "None"
    \end{itemize}
    \end{enumerate}

\begin{table}[htb]
    \centering
    \makebox[\textwidth]{\includegraphics[width=1.2\textwidth]{graphics/evaldata.pdf}}
    \caption{Format and Example Entries of the Evaluation Dataset}
    \label{tab:dataset-format}
\end{table}
    
Based on the chat history a request can be build with a python script leveraging langchain to easily access Ollama since it is supported by langchain.
That way we do not have to construct API calls ourself only parse the message history into a langchain function call that does everything in the background.
We can just create an ChatOllama object like the following:

\begin{lstlisting}[numbers=none, frame=none]
# Initialize language model
llm = ChatOllama(
    base_url="http://127.0.0.1:5000",
    model="sh-llama3-instruct",
    keep_alive=-1
)
\end{lstlisting}

Therefore we can easily switch between all models that we want to test out. The keep alive option set to minus one means that the model is loaded undefinetly into memory.
After parsing the messages of one csv entry we can just use the following code to create the code and invoke the language model:

\begin{lstlisting}[numbers=none, frame=none]
prompt = ChatPromptTemplate.from_messages(messages)
result = invoke_language_model(llm, prompt)
\end{lstlisting}

To create a diverse and representative dataset, we used 10 example device lists as the basis for our scenarios. These lists were carefully crafted to cover various smart home setups:

\begin{itemize}
    \item 5 edge case device lists, including:
    \begin{itemize}
    \item Thermostats in different rooms
    \item Multiple thermostats in the same room
    \item Multiple smart plugs with similar names in the same room
    \item Multiple door/window contacts in the same room
    \item A setup where the user has no thermostats
    \end{itemize}
    \item 3 random German examples with device and room names in German
    \item 2 random English examples with typical device names and variations of supported devices
\end{itemize}

These device lists were sometimes modified (e.g., changing variable values) to match specific test cases, ensuring a wide range of scenarios for evaluation.
The dataset covers various interaction types, including device control commands, queries about device states, requests for information about the smart home setup, and complex questions requiring reasoning about multiple devices or rooms.
Table \ref{tab:dataset-format} illustrates the format of the dataset and provides two example entries (note that the device lists are shortened to one device and removed state information here for a better overview, the actual device lists contain 2-6 devices).

This carefully created dataset allows us to evaluate the chatbot's performance across multiple dimensions, including accuracy in interpreting user intent, ability to provide relevant responses, correct identification and execution of required actions, contextual understanding, and handling of edge cases and ambiguous requests.


\subsection{Evaluation Metrics}
In this section we describe the metrics we used to measure the accuracy of different language models that we customized with our modelfile as described in \cref{subsec:modelcust}.
For this we have more deeply analyzed the metrics stated in the related work \cref{sec:relatedeval}.
We employed a combination of metrics that evaluate both the semantic accuracy of the natural language responses and the correctness of the generated JSON commands to reliably capture the relevant dimensions of our chatbot prototype.

\subsubsection{Semantic Similarity}

Semantic similarity is a crucial metric in our evaluation since it measures how closely the generated responses from the chatbot match the expected outputs in terms of meaning. For this evaluation, we used the \texttt{SentenceTransformer} model, specifically the \texttt{paraphrase-MiniLM-L6-v2} variant, to compute cosine similarity between the embeddings of the generated responses and the expected outputs. A high similarity score indicates that the chatbot's response is semantically close to the expected answer, even if the exact wording differs.

The process of calculating semantic similarity involves the following steps: \\
Encoding: Both the reference (expected) outputs and the generated responses are encoded into high-dimensional vectors using the SentenceTransformer model.\\
Similarity Computation: The cosine similarity between the embeddings of the reference and generated responses is calculated. Cosine similarity measures the cosine of the angle between two vectors, providing a value between -1 and 1, where 1 indicates perfect similarity, 0 indicates no similarity, and -1 indicates perfect dissimilarity.\\
Aggregation: The individual similarity scores are aggregated to produce an average similarity score across all samples in the evaluation dataset.\\

\begin{Listing}[htb]
    \begin{lstlisting}[language=Python]
def calculate_semantic_similarity(references, generated_responses):
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    embeddings1 = model.encode(references, convert_to_tensor=True)
    embeddings2 = model.encode(generated_responses, convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)

    similarities = [cosine_scores[i][i].item() for i in range(len(references))]
    average_similarity = sum(similarities) / len(similarities)
    return similarities, average_similarity
  \end{lstlisting}
    \caption{Code for calculating the semantic similarity through cosine similarity}
    \label{lst:similarity}
\end{Listing}

The implementation of this metric is shown in \cref{lst:similarity}. This code defines a function \texttt{calculate\_semantic\_similarity} that takes two lists of sentences (references and generated responses) as input and returns a list of individual similarity scores along with the average similarity score.
A high average similarity score indicates that the chatbot's responses are semantically close to the expected answers, even if the exact wording differs. This allows for a more flexible evaluation that captures the chatbot's ability to understand and respond to user intents accurately, rather than merely reproducing exact phrases.


\subsubsection{JSON Accuracy}
We define \gls{json} accuracy as the percent of correctly generated \glspl{json} by a language model customized to our use case.
Since our dataset contains only necessary keys of the expected \gls{json}, a correct generated \gls{json} is one which contains each expected key and the correct corresponding value for it.

Each generated \gls{json} is compared against the expected \gls{json} to determine if it correctly represents the intended action or response. 
The code in \cref{lst:evalMetrics1} shows how we have implemented this. The accuracy we just explained is represented by the variable ``accuracy''.

The total count (total\_count) is the total number of generated responses, which represents the total number of \glspl{json} evaluated. This is determined by the length of the generated\_responses list.
The correct\_count is increased in several scenarios:
\begin{enumerate}
    \item When both the expected and generated \glspl{json} are None.
    \item When the expected \gls{json} is None and the generated \gls{json} has an "action" key with the value "none".
    \item When the generated \gls{json} matches the expected \gls{json} in terms of keys and their corresponding values (determined by the compare\_jsons function).
\end{enumerate}
Generated \glspl{json} that would throw an error on parsing are handled as incorrect. This is implemented in the code through the use of try-except blocks. If a JSONDecodeError occurs when trying to parse the expected \gls{json}, or if an AttributeError occurs when comparing the generated and expected \glspl{json}, the \gls{json} is considered incorrect and the json\_accuracy\_flags for that instance is set to False.
The accuracy is then calculated by dividing the correct\_count by the total\_count.

The code also includes the variable ``key\_accuracy'' which checks how many keys have the correct value as in the expected \gls{json}. It is calculated as follows:
\begin{itemize}
    \item total\_keys is incremented for each key in the expected \gls{json}.
    \item correct\_keys is incremented when a key in the generated \gls{json} matches the corresponding key in the expected \gls{json}.
    \item The key\_accuracy is then calculated as the ratio of correct\_keys to total\_keys.
\end{itemize}
This key\_accuracy provides a more granular measure of how well the generated \glspl{json} match the expected \glspl{json} on a key-by-key basis, even if the entire \gls{json} doesn't match perfectly.

The function shown returns three values: the overall \gls{json} accuracy, the key accuracy, and a list of boolean flags indicating which generated \glspl{json} were correct (json\_accuracy\_flags).

\begin{Listing}
    \begin{lstlisting}[language=Python]
def evaluate_jsons(generated_responses, generated_jsons, expected_json_values):
    correct_count = total_keys = correct_keys = 0
    total_count = len(generated_responses)
    json_accuracy_flags = []

    for response, generated_json, expected_json in zip(generated_responses, generated_jsons, expected_json_values):
        if expected_json is not None and isinstance(expected_json, str):
            try:
                expected_json = json.loads(expected_json)
            except json.JSONDecodeError:
                json_accuracy_flags.append(False)
                continue        
        if expected_json is None and generated_json is None:
            correct_count += 1
            json_accuracy_flags.append(True)
            continue
        if expected_json is None:
            if generated_json.get("action") == "none": 
                correct_count += 1
                json_accuracy_flags.append(True)
                continue
            json_accuracy_flags.append(False)
            continue        
        if generated_json is None:
            json_accuracy_flags.append(False)
            continue
        try:
            keys_correct = compare_jsons(generated_json, expected_json)
            if keys_correct:
                correct_count += 1
                json_accuracy_flags.append(True)
            else:
                json_accuracy_flags.append(False)
            
            for key in expected_json:
                total_keys += 1
                if generated_json.get(key) == expected_json.get(key):
                    correct_keys += 1
        except AttributeError:
            json_accuracy_flags.append(False)
    
    accuracy = correct_count / total_count
    key_accuracy = correct_keys / total_keys if total_keys > 0 else 0
    return accuracy, key_accuracy, json_accuracy_flags
  \end{lstlisting}
    \caption{Code for Classificiation of the models responded JSONs}
    \label{lst:evalMetrics1}
\end{Listing}

\begin{Listing}
    \begin{lstlisting}[language=Python]
def normalize_value(value):
    """Normalize the value for comparison."""
    try:
        # Try to convert strings that represent numbers to float
        return float(value)
    except (ValueError, TypeError):
        # If it's not a number or it's already a number, return it as is
        return value

def compare_jsons(generated_json, expected_json):
    """Compare two JSON objects with normalized values."""
    if generated_json is None or expected_json is None:
        return generated_json == expected_json
    
    for key in expected_json:
        if key not in generated_json:
            return False
        # normalize value if the key is "value"
        if key == "value":
            return normalize_value(generated_json[key]) == normalize_value(expected_json[key])
        else:
            return generated_json[key] == expected_json[key]
    return True
    \end{lstlisting}
    \caption{Code for comparing actual and expected JSONs}
    \label{lst:compare-json}   
\end{Listing}


\subsection{Combined Metric}

To provide a holistic view of the chatbot's performance, we developed a combined metric that integrates both semantic similarity and \gls{json} accuracy. This approach was inspired by the need to evaluate the chatbot's performance across multiple dimensions simultaneously.
The combined metric adapts the concepts of precision, recall, and F1 score from traditional classification tasks to our specific use case. Here's how we define the components:

\gls{tp}: Cases where the model's output has high semantic similarity (above a defined threshold) and the generated \gls{json} is correct.\\
\gls{fp}: Cases where the model's output has high semantic similarity but the generated \gls{json} is incorrect.\\
\gls{tn}: Cases where the model's output has low semantic similarity and the  generated \gls{json} is incorrect.\\
\gls{fn}: Cases where the model's output has low semantic similarity but the generated \gls{json} is correct.

Based on these definitions, we calculate precision, recall, and F1 score as follows:

\paragraph{Precision:} Of all the outputs the model that have a high semantic similarity, how many were actually positive cases (generated \gls{json} is also correct).
\paragraph{Recall:} Of all the actual positive cases (generated \gls{json} is correct), how many did the model correctly identify as positive (semantic similarity is also high).
\paragraph{F1 Score:} The harmonic mean of precision and recall, providing a single measure of performance.

The implementation of this combined metric is shown in \cref{lst:classificationRefined} and based on the scikit-learn library\footnote{\url{https://scikit-learn.org/stable/modules/model\_evaluation.html\#precision-recall-and-f-measures}}. This function, \texttt{calculate\_classification\_metrics}, takes lists of similarity scores and \gls{json} accuracy flags as input, along with a similarity threshold. It then computes the precision, recall, and F1 score based on our adapted definitions.
It's important to note that this approach, while not standard for non-classification tasks, provides valuable insights into our chatbot's performance. By combining semantic similarity and \gls{json} accuracy, we can evaluate how well the model meets both criteria simultaneously, which is crucial for its functionality in a smart home system.
The similarity threshold is a critical parameter that determines what constitutes "high" semantic similarity. This threshold should be chosen based on domain knowledge and experimentation to ensure that the similarity measure is robust and meaningful for the specific use case.
By using this combined metric, we can quantify the chatbot's ability to provide semantically appropriate responses while also generating accurate \gls{json} commands. This approach aligns well with the practical expectations of the chatbot's performance in a real-world smart home setting.

\begin{Listing}
    \begin{lstlisting}[language=Python]
    def calculate_classification_metrics(similarities, json_accuracy_flags, similarity_threshold=0.8):
    y_true = []
    y_pred = []

    for similarity, json_correct in zip(similarities, json_accuracy_flags):
        # True label is positive if JSON is correct
        y_true.append(1 if json_correct else 0)

        # Predicted positive if similarity is above threshold and JSON is correct
        if similarity >= similarity_threshold and json_correct:
            y_pred.append(1)
        else:
            y_pred.append(0)

    # Calculate precision, recall, and F1 score
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return precision, recall, f1
  \end{lstlisting}
    \caption{Refined Classification Metrics}
    \label{lst:classificationRefined}
\end{Listing}

\subsubsection{Determining the Similarity Threshold}
The choice of an appropriate similarity threshold is crucial for the effectiveness of our combined metric. Through careful analysis of the semantic similarity between various generated outputs and their corresponding expected outputs, we determined that a threshold of 0.65 provides the most meaningful differentiation. This decision was based on several key observations:

\textbf{Cross-language Differentiation}: We found that when a generated response was in English but the expected response was in German, the similarity score was consistently below 0.55. This served as a clear demarcation point, even when considering the constraints of the language model size we chose.

\textbf{Quality Assessment}: After establishing 0.55 as a baseline, we examined cases with higher similarity scores. Our analysis revealed that responses with a similarity score greater than 0.65 were consistently of high quality and relevance.

\textbf{Borderline Cases}: We identified examples that fell just below and above our chosen threshold to validate its appropriateness. For instance, an unacceptable response with a similarity of approximately 0.61 was: "I cannot have a temperature measurement for the kitchen. There are only rooms and devices, but no temperature-taking devices available for the kitchen." The expected response was: "I'm sorry, but no temperature sensors are available in the kitchen." Despite some semantic overlap, this response was deemed insufficient.
It is worth noting that this is a literal translation from a German example, i.e., when calculating the similarity between those two sentences it will probably differ from the actual one.

\textbf{Acceptable Responses}: Conversely, we found that responses just above the 0.65 threshold were generally acceptable. An example with a similarity of about 0.67 was: "The temperature set point is 18 degrees, and the current temperature is 19 degrees." The expected response was: "The temperature in the Schlafzimmer is set to 18.0 degrees Celsius and the current temperature is 19.0 degrees Celsius." This response, while missing some specifics, captured the essential information accurately.

By setting the threshold at 0.65, we strike to conduct a strict and reliable evaluation. This threshold effectively distinguishes between responses that capture the core meaning and intent of the expected output and those that fall short, providing a robust basis for our combined metric.



\section{User Experience}

To evaluate the user experience of our smart home chatbot, we designed and conducted a comprehensive user study. This study aimed to assess the usability, effectiveness, and potential benefits of integrating a chatbot interface into a smart home system.

\subsection{Participants}
We recruited a diverse group of 10 participants for our study, comprising three employees from Bosch Smart Home and seven external participants, including university students and employees from other companies. The participants represented a varied demographic in terms of age and professional background, with 60\% having a technical background and 40\% non-technical. Experience with smart home systems varied among participants, with only one having no prior experience. Familiarity with chatbots was evenly distributed across the spectrum from no experience to regular use.

This diverse group was chosen to represent a range of potential users, from those familiar with smart home technology to novices, ensuring a comprehensive evaluation of the chatbot's accessibility and usefulness.

\subsection{Apparatus}
The experimental setup consisted of three smart home devices: a smart plug named "TV" and a thermostat, both located in the sleeping room, and a door/window contact in the living room. The language model ran on a PC with specifications described in Section [reference to hardware specs]. Participants interacted with the Bosch smart home app either through a PC emulating Android or a Samsung Galaxy S21 Smartphone. To ensure consistency, all participants used the same setup in the same room.

\subsection{Procedure}
Participants were asked to complete six tasks using the chatbot interface: inquiring about the chatbot's purpose, determining the state and energy consumption of the TV, checking temperature settings in the bedroom, requesting a summary of available devices, changing the bedroom temperature, and switching the TV's state. To mitigate learning effects, the order of these tasks was randomized for each participant.

\subsection{Metrics and Data Collection}
We collected both quantitative and qualitative data to evaluate the user experience. Quantitative metrics included task completion time, number of attempts per task, and task success rate. These were collected through automated logging and researcher observation. Qualitative data was gathered through a questionnaire using a 5-point Likert scale to assess various aspects of the chatbot's usability and effectiveness, followed by a semi-structured interview to explore participants' experiences in more depth.

\subsection{Analysis}
The data analysis plan incorporated both quantitative and qualitative approaches. Quantitative analysis focused on task completion rates, time taken, and survey scores. Qualitative analysis of interview responses aimed to identify common themes and areas for improvement. This mixed-methods approach allows for a comprehensive evaluation of the chatbot's performance and user perception, providing both statistical measures of effectiveness and rich, contextual insights into the user experience.


\section{Results}

\subsection{Language Model Performance Results}

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{\textbf{Model}} & \multicolumn{5}{c|}{\textbf{Semantic Similarity Values}} & \multicolumn{2}{c|}{\textbf{JSON Accur}} & \multicolumn{3}{c|}{\textbf{Combined Metrics}} \\ \cline{2-11}
             & \textbf{Mean} & \textbf{Median} & \textbf{Std Dev} & \textbf{Range} & \textbf{>0.65} & \textbf{Total} & \textbf{Key} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} \\ \hline
            shllama3 & 0.5667 & 0.5916 & 0.2608 & 1.0737 & 40.24\% & 0.7805 & 0.5789 & 1 & 0.47 & 0.64 \\
            shllama3instr & 0.6036 & 0.6758 & 0.2878 & 1.0885 & 51.22\% & 0.8049 & 0.8571 & 1 & 0.5606 & 0.7184 \\
            shllama3-2 & 0.4185 & 0.3909 & 0.2660 & 1.0654 & 19.51\% & 0.8171 & 0.7037 & 1 & 0.1940 & 0.3250 \\
            shllama3instr-2 & 0.3888 & 0.3857 & 0.2314 & 1.1155 & 13.41\% & 0.8780 & 0.6 & 1 & 0.125 & 0.2222 \\ \hline
        \end{tabular}
    }
    \caption{Descriptive Statistics and Analysis of the semantic similarity and performance metrics for the different llama3 models created.}
    \label{tab:descr-model}
\end{table}

In this section we want to show our results from four variations that we created from the \texttt{llama3} model. Even though we tested all models mentioned in cref{subsec:modelcust} only the llama3 model was worth evaluating as described before in \cref{subsec:hardware-llm}.
what we can see in this table are the four different versions of \texttt{llama3}. Note that we abbreviated the names of our model versions in the table to have a better table layout. Also, the ``sh'' stands for ``Smart Home''. We used the same modelfile for \texttt{shllama3} and \texttt{shllama3instruct} and after analyzing use cases that it can not handle and tried to build an improved modelfile that we used to create \texttt{shllama3instr-2} and \texttt{shllama3instr-2}.
The initial versions of the models encountered challenges in specific scenarios, including:
Generation of multiple \glspl{json} in a single response which was not supported by our prototype,
inconsistent selection of language to answer in,
inaccurate calculation of average temperatures and
inconsistent handling of temperature increase/decrease by a certain value commands.
To address these issues, we developed improved modelfiles for the second generation models (shllama3-2 and shllama3instr-2) that included more precise descriptions and additional example conversations.


In the table we can see that the revised modelfiles for the second generation of models we created did improve the total \gls{json} accuracy, but for the \texttt{shllama3instr-2} the key accuracy decreased. For the revised modelfiles also suffer in the overall quality in responses which according to the semantic similarity are much worse with a Mean around 0.4.
This of course hardly effects the combined metrics with an F1 score of 0.3250 and 0.2222 which is barely half as much as for the first generation models. This means that the models are quite unpredictable or random in their responses which can be seen especially in the semantic similarity values.
The results demonstrate a trade-off between JSON accuracy and semantic similarity. While the second-generation models showed improved JSON accuracy, they suffered a significant decrease in semantic similarity. This suggests that the modifications made to improve specific functionalities may have compromised the models' overall language understanding and generation capabilities.

\begin{figure}[H]
    \centering
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/shllama3-semantic-distribution.png}
        \caption{Distribution of Similarity Scores for shllama3}
        \label{fig:shllama3}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/shllama3instruct-semantic-distribution.png}
        \caption{Distribution of Similarity Scores for shllama3instruct}
        \label{fig:shllama3instruct}
    \end{minipage}
    \vskip\baselineskip
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/shllama3-2-semantic-distribution.png}
        \caption{Distribution of Similarity Scores for shllama3-2}
        \label{fig:shllama3-2}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/shllama3instruct-2-semantic-distribution.png}
        \caption{Distribution of Similarity Scores for shllama3instruct-2}
        \label{fig:shllama3instruct-2}
    \end{minipage}
    \caption{Distribution of Similarity Scores for Different Models}
    \label{fig:similarity-distributions}
\end{figure}

For individually considering the semantic similarity of the models we created histograms that are shown in \cref{fig:similarity-distributions}. They futher highlight the better performance of our first generation models \texttt{shllama3} and \texttt{shllama3instruct} since the values for those less values are distributed below a similarity score of 0.4 or even 0.5 than the first generation models.
Especially \texttt{shllama3instruct} shows a good distribution with most values being at least 0.6. Approximately twice as many values are above 0.6 than below.

To further compare the semantic similarity values between the models \texttt{shllama3} and \texttt{shllama3instruct}, we also performed a Wilcoxon signed-rank test. This non-parametric test is used to determine whether there is a statistically significant difference between the paired samples from these two models and has the benefit that we do not assume normality in the distribution of differences like in the typical t-test method. When we have a look at the previous distributions of similarity scores we would assume that no normal distribution is given.
The results of the Wilcoxon signed-rank test are as follows:
\begin{itemize}
    \item \textbf{Wilcoxon signed-rank test statistic}: 1395.0
    \item \textbf{P-value}: 0.1565
\end{itemize}
Given that the p-value (0.1565) is greater than the commonly used significance level of 0.05, we fail to reject the null hypothesis. This indicates that there is no statistically significant difference in the distribution of semantic similarity values between the two models, \texttt{shllama3} and \texttt{shllama3instruct}, at the 0.05 significance level (even if we would use a less strict significance level of 0.1 which could be applicable for comparing the semantic similarity of language models).
In practical terms, these results suggest that both models perform similarly in terms of semantic similarity for the given data set.
However, it's important to note that while not statistically significant, there is a noticeable difference in the median values and the percentage of responses above the 0.65 threshold between shllama3 and shllama3instruct. This suggests that shllama3instruct may offer some practical improvements in semantic similarity, even if not statistically significant at the conventional 0.05 level.

To further compare the JSON correctness between the models \texttt{shllama3} and \texttt{shllama3instruct}, we performed McNemar's test. This test is used to evaluate differences on paired binary data, which in this case is whether the generated JSON was correct (1) or not (0).
The contingency table for the JSON correctness is \cref{tab:conti} and shows that both models correctly generated the expected \gls{json} in 56 cases. \texttt{shllama3instruct} correctly generated 10 \glspl{json} that \texttt{shllama3} did not and vice versa \texttt{shllama3} generated 8 correct that \texttt{shllama3instruct} did not.
\begin{table}[h]
    \centering
    \begin{tabular}{cc|c|c}
        & & \multicolumn{2}{c}{\texttt{instruct}} \\
        & & 0 & 1 \\ \hline
        \multirow{2}{*}{\texttt{shllama3}} & 0 & 8 & 10 \\ 
        & 1 & 8 & 56 \\ 
    \end{tabular}
    \caption{Contingency table for JSON correctness between \texttt{shllama3} and \texttt{shllama3instruct}.}
    \label{tab:conti}
\end{table}

The results of McNemar's test are as follows:
\begin{itemize}
    \item \textbf{McNemar's test statistic}: 8.0
    \item \textbf{P-value}: 0.8145
\end{itemize}
Given that the p-value (0.8145) is greater than the significance level of 0.05, we fail to reject the null hypothesis. This indicates that there is no statistically significant difference in JSON correctness between the two models. Both models exhibit similar performance in generating correct JSON responses.
Despite the lack of statistical significance, it's worth noting that shllama3instruct correctly generated 10 JSONs that shllama3 did not, while the reverse occurred only 8 times. This slight edge, although not statistically significant, might indicate a marginal improvement in JSON generation capabilities for shllama3instruct.

Another important part of the results is whether the models generated \glspl{json} that would trigger an action in the smart home when none was expected or generated no \gls{json} when an \gls{json} was expected.
In \cref{tab:json_summary} we can see that \texttt{shllama3} has the most mistakes in total for this metric while shllama3instruct-2 has the least which also has no unexpectedly generated \gls{json}.
In contrast \texttt{shllama3-2} generated the most unexpected \glspl{json}.

\begin{table}[h!]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Model & Generated JSON (unexpected) & No JSON (but expected one) \\
        \midrule
        shllama3       & 4 & 4 \\
        shllama3instruct  & 3 & 3 \\
        shllama3-2     & 5 & 0 \\
        shllama3instruct-2& 0 & 4 \\
        \bottomrule
    \end{tabular}
    \caption{Generated JSONs vs Expected JSONs}
    \label{tab:json_summary}
\end{table}

In conclusion, our analysis reveals that while attempts to improve specific functionalities in the second-generation models led to increased JSON accuracy, this came at the cost of reduced semantic similarity and overall performance. The first-generation models, particularly shllama3instruct, demonstrate a better balance between semantic understanding and JSON generation. These findings highlight the challenges in optimizing language models for specific tasks while maintaining general language understanding capabilities.


\subsection{Quantitative User Study Results}
% hypothesises: 1. controlling devices tasks generally take longer than tasks were no action needs to be triggered, 2. persons with technical background need less time for the tasks, 3. persons without technical background see more potential in the chatbot 4. Older persons are more concerned about the chatbot 5. The chatbot improves explainabiltiy of the app 6. The chatbot improves usability of the app
%
%
In this section we want to state our results for the quanitative part of our user study. For this we present the following hypothesises:
\begin{itemize}
    \item[\(H_1\)] The chatbot improves explainability of the app.
    \item[\(H_2\)] The chatbot improves usability of the app.
    \item[\(H_3\)] Persons would rather use the chatbot than traditional ways of controlling their smart home.
    \item[\(H_4\)] Controlling device tasks generally take longer than tasks where no action needs to be triggered.
    \item[\(H_5\)] The chatbot's ability to handle complex queries involving multiple devices is perceived as more valuable than its performance on simple, single-device tasks.
\end{itemize}



\subsection{Qualitative User Study Results}
The semi-structured interviews yielded rich qualitative data, which we analyzed using thematic analysis. Four main themes emerged from the participants' responses to the questions about their experience with the smart home chatbot:
\subsubsection{Chatbot Limitations and Frustrations}
Many participants reported frustrations with the chatbot's limitations. Common issues included the chatbot's inability to control devices as expected, misinterpretation of complex queries, and difficulty handling follow-up questions or maintaining appropriate context. One participant noted, "When [the chatbot] should control devices and it doesn't do it," while another mentioned, "Sometimes the chatbot misinterprets complex queries."
\subsubsection{User Interface and Interaction Preferences}
Participants expressed various preferences and suggestions for improving the chatbot's user interface and interaction methods. These included requests for example commands, voice input options, and more visual elements. One participant suggested, "Example of possible questions would be nice to show. A voice option would be nice. Maybe more style in the text bold etc." Another recommended "more visual elements like icons or graphics."
\subsubsection{Utility for Complex Tasks and Device Management}
Many participants found the chatbot particularly useful for complex tasks involving multiple devices or for obtaining quick overviews of their smart home system. One participant noted the chatbot would be "very helpful if I had many devices (50 windows, outlets), then questions like 'is any window open' [would be useful]." Another mentioned its utility "for complex automations and scenarios involving multiple devices."
\subsubsection{Interest in Advanced Features and Optimization}
Participants showed significant interest in advanced features, particularly those related to energy optimization and understanding system behavior. One participant expressed interest in "automatic suggestions/tips for optimizing behavior (usage patterns, power consumption...)." Another was "interested in understanding the logic behind my smart home's operations."

\subsection{User Experience Demonstration}
% include screenshots of example conversations 

\section{Discussion}
% not always response in correct language --> could be solved in telling the model in 
% which language to answer through the system language of the phone/app

% This work shows how a chatbot application for smart homes can be built. It could be transferred into a framwework where it would only be necessary to specify the domain to a self hosted language model or an commercial API with API key and the mapping functionality for parsing and mapping the output \gls{json} of the language model to the actions of the smart home system

... Based on the results argue about acceptance or rejection of your research hypothesis   .. 
Interpretation of Results
Comparison with Previous Studies
Limitations of the Study

----------------------------------------------------------------
Discussion:
-The \texttt{shllama3instr} version is by far the best customization of a model we were able to create even though both statistical tests we conducted showed no significant differences to the \texttt{shllama3} vresion. The semantic similarity is overall much better than every other model version when we look at especially at the similarity values percentage that was greater than 0.65. It also has the highest F1 score which is with over 70\% a good starting point. With fine tuning the model will probably perform even better for the intents we were able to implement. 
-Mistakes will always happen, but for out chatbot it is more critical to generate a \gls{json} that would trigger an action when none is expected than the other way around. \texttt{shllama3-2} was the words model according to this rule and \texttt{shllama3instr-2} the best with no unexpectedly generated \gls{json}. However, the semantic similarity is much worse than in every other model and really makes the model unsuitable.

-For \texttt{shllama3instr-2} the \gls{json} key accuracy is so bad because even though it is able to generate the correct \gls{json} in more complex scenarios sometimes hallucinates a \gls{json} were no \gls{json} is needed which directly influences the key accuracy.

-The second generation models probably got worse because the models got stuck with too much examples shown in the revised modelfiles and only tries to repeat the examples instead of understanding the structure of generic user requests, it just doesn't understand the provided device list. Maybe the device data should be preprocessed to a more readable format like (Device List:
%1. Type: POWER\_METER\_SWITCH, Name: Zwischenstecker, Room: Schlafzimmer
   - Power consumption: 0, Energy consumption: 0
   - Switch state: OFF
%2. Type: SHUTTER\_CONTACT, Name: Fenster Esszimmer, Room: Wohnzimmer
   - Contact state: CLOSED
...) since Each device has multiple attributes and nested state information, which adds to the complexity. Other approaches could be: Explicit instructions: Include clear instructions on how to use the device list. For example:
"The following is a list of smart home devices. Use this information to answer user queries about device states and controls."
Separate message: Instead of including the device list in the user message, you could create a separate system message containing this information.

-compared to the discussed work of \citet{acon96_home_llm} who claims a ``97.11\% score for JSON function calling accuracy'' for his most recent model ``Home-3B-v3'' while we achieved an accuracy of 80.49\% with the model we selected as having the best overall performance. While our model seems to perform significantly worse on this accuracy our model is proabably able to answer more complex questions and intentions of users. Another point is that we can not really compare the quality of answers since no metrics are stated on this for the ``Home-3B-v3''. Also the purpose of this model is limited to achieve perfect accuracy in controlling devices while our purposes are much more diversified. 

-It is interesting that out of all models we selected only llama3 was able to understand the logic behind our approach but other models were not, especially when we take in cosinderation that we even tested much bigger models. It could be that llama3 is more deeply trained on \gls{json} data

-The problem with the language not matching the language of a users message also occured when trying around with other models than llama3. So it is either a typical problem in the parameter sizes of models we used or because the provided device list is in english.
 
----------------------------------------------------------------

\subsection{Interpretation of Qualitative Findings}
The qualitative data from our study provides valuable insights into users' experiences with the smart home chatbot and their expectations for such a system.
\subsubsection{Balancing Simplicity and Complexity}
Our findings reveal a tension between users' desire for simplicity and their interest in complex functionalities. While some participants found technical language challenging and requested simpler interactions, others were eager for advanced features like energy optimization and complex automations. This suggests that future iterations of the chatbot should offer tiered interaction levels, allowing users to choose between simplified and more advanced interfaces based on their comfort and expertise.
\subsubsection{Enhancing Contextual Understanding}
The frustrations expressed regarding the chatbot's contextual understanding and ability to handle complex queries highlight an area for significant improvement. Implementing more sophisticated natural language processing and context retention could greatly enhance the user experience. As one participant suggested, a feature to correct the chatbot's understanding mid-conversation could be particularly valuable.
\subsubsection{Leveraging Visual and Multi-modal Interactions}
The requests for more visual elements and voice input options indicate that users desire a more diverse and intuitive interaction experience. Incorporating these elements could make the chatbot more accessible and engaging for a wider range of users, potentially increasing its adoption and regular use.
\subsubsection{Focusing on High-Value Use Cases}
The strong interest in using the chatbot for complex tasks, device management, and system optimization suggests these are high-value use cases that should be prioritized in future development. The chatbot's ability to handle scenarios involving multiple devices and provide quick system overviews appears to be a significant advantage over traditional interfaces.
\subsubsection{Addressing Privacy and Scope Concerns}
While many participants were enthusiastic about advanced features, some expressed concerns about privacy and the appropriate scope of the chatbot's functionality. This highlights the need for transparent data handling practices and clear communication about the chatbot's capabilities and limitations.
\subsection{Implications for Future Development}
Based on these findings, future development of the smart home chatbot should focus on:
\begin{enumerate}
    \item Improving natural language processing to handle more complex and contextual queries.
    \item Developing a tiered interface that caters to both novice and advanced users.
    \item Incorporating more visual elements and possibly voice interaction capabilities.
    \item Enhancing capabilities for managing multiple devices and creating complex automations.
    \item Implementing features for energy optimization and system behavior insights.
    \item Ensuring robust privacy protections and clear communication about data usage.
\end{enumerate}


These enhancements could significantly improve user satisfaction and the overall utility of the chatbot in smart home environments.


\section{Threats to Validity}
% all participants had either interest in the product by working at Bosch Smart Home or be known by the researchers. Therefore positive resonance may be biased.
% mistakes in the evaluation datasets could occur since it was quite complex to build. - Lead to both: artificially better or worse results
%  classification through the work here? ... Discuss what threatens validity of your result. In case you could counteract them explain how. For experimentation in software engineering there is already a classification of this threats and a check-list \cite{DBLP:journals/ese/RunesonH09}.
% A very case that however can influence the results is the following: The model generates multiple jsons (which was not supported by our prototype). Than our evaluation script puts an error as generated json even though the generated jsons by the model were actually all including a ``none action'' and would therfore do nothing which is correct. Also the similarity is quite good but in the end this would be classified as negative case. "Generated: I can't find a thermostat device installed. So I can't change the temperature. Expected: "I can't find any device to control the temperature.,	Generated JSON: {"error": "response contains multiple JSONs which is invalid"},	Expected Json: null	 Similarity: 0.7633,	is_json_correct: FALSCH



%LaTeX-Hinweise stehen in \cref{chap:latexhints}.

%noch etwas Fülltext
%\blinddocument
