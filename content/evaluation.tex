% !TeX spellcheck = en_US
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    %backgroundcolor=\color{gray!10},
    frame=lr,
    %captionpos=b,
    tabsize=2,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green},
    stringstyle=\color{red},
    lineskip=0.1em  % Add space between lines
}

\chapter{Evaluation}
\label{chap:evaluation}

In this chapter, we present the evaluation methodology and results for the smart home chatbot. The evaluation focuses on two key aspects: the semantic similarity of the responses and the accuracy of the generated JSON commands. Additionally, we discuss the initial approach using classification metrics, the challenges encountered, and the refined approach to address these challenges.

\section{Evaluation Dataset}


\section{Evaluation Metrics}

\subsection{Semantic Similarity}

Semantic similarity measures how closely the generated responses from the chatbot match the expected outputs in terms of meaning. For this evaluation, we used the \texttt{SentenceTransformer} model, specifically the \texttt{paraphrase-MiniLM-L6-v2} variant, to compute cosine similarity between the embeddings of the generated responses and the expected outputs. A high similarity score indicates that the chatbot's response is semantically close to the expected answer, even if the exact wording differs.

\begin{Listing}
    \begin{lstlisting}[language=Python]
def calculate_semantic_similarity(references, generated_responses):
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    embeddings1 = model.encode(references, convert_to_tensor=True)
    embeddings2 = model.encode(generated_responses, convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)

    similarities = [cosine_scores[i][i].item() for i in range(len(references))]
    average_similarity = sum(similarities) / len(similarities)
    return similarities, average_similarity
  \end{lstlisting}
    \caption{Code for calculating the semantic similarity through cosine similarity}
    \label{lst:similarity}
\end{Listing}


\subsection{JSON Accuracy}

JSON accuracy evaluates the correctness of the structured data outputs generated by the chatbot. Each generated JSON is compared against the expected JSON to determine if it correctly represents the intended action or response. The accuracy is calculated by the proportion of correct JSONs to the total number of JSONs evaluated.

\begin{Listing}
    \begin{lstlisting}[language=Python]
def evaluate_jsons(generated_responses, generated_jsons, expected_json_values):
    correct_count = total_keys = correct_keys = 0
    total_count = len(generated_responses)
    json_accuracy_flags = []

    for response, generated_json, expected_json in zip(generated_responses, generated_jsons, expected_json_values):
        if expected_json is not None and isinstance(expected_json, str):
            try:
                expected_json = json.loads(expected_json)
            except json.JSONDecodeError:
                json_accuracy_flags.append(False)
                continue        
        if expected_json is None and generated_json is None:
            correct_count += 1
            json_accuracy_flags.append(True)
            continue
        if expected_json is None:
            if generated_json.get("action") == "none": 
                correct_count += 1
                json_accuracy_flags.append(True)
                continue
            json_accuracy_flags.append(False)
            continue        
        if generated_json is None:
            json_accuracy_flags.append(False)
            continue
        try:
            keys_correct = compare_jsons(generated_json, expected_json)
            if keys_correct:
                correct_count += 1
                json_accuracy_flags.append(True)
            else:
                json_accuracy_flags.append(False)
            
            for key in expected_json:
                total_keys += 1
                if generated_json.get(key) == expected_json.get(key):
                    correct_keys += 1
        except AttributeError:
            json_accuracy_flags.append(False)
    
    accuracy = correct_count / total_count
    key_accuracy = correct_keys / total_keys if total_keys > 0 else 0
    return accuracy, key_accuracy, json_accuracy_flags
  \end{lstlisting}
    \caption{Code for Classificiation of the models responded JSONs}
    \label{lst:evalMetrics1}
\end{Listing}

\begin{Listing}
    \begin{lstlisting}[language=Python]
def normalize_value(value):
    """Normalize the value for comparison."""
    try:
        # Try to convert strings that represent numbers to float
        return float(value)
    except (ValueError, TypeError):
        # If it's not a number or it's already a number, return it as is
        return value

def compare_jsons(generated_json, expected_json):
    """Compare two JSON objects with normalized values."""
    if generated_json is None or expected_json is None:
        return generated_json == expected_json
    
    for key in expected_json:
        if key not in generated_json:
            return False
        # normalize value if the key is "value"
        if key == "value":
            return normalize_value(generated_json[key]) == normalize_value(expected_json[key])
        else:
            return generated_json[key] == expected_json[key]
    return True
    \end{lstlisting}
    \caption{Code for comparing actual and expected JSONs}
    \label{lst:compare-json}   
\end{Listing}

\section{Initial Approach Using Classification Metrics}

\subsection{Description of the Approach}

Initially, we attempted to evaluate the chatbot using traditional classification metrics: precision, recall, and F1 score. In this context, we defined the true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) based on the correctness of the JSON outputs and the semantic similarity scores.

\begin{lstlisting}[language=Python, caption=Classification Metrics]

\end{lstlisting}

\begin{Listing}
    \begin{lstlisting}[language=Python]
def calculate_classification_metrics(similarities, json_accuracy_flags, similarity_threshold=0.8):
    y_true = []
    y_pred = []

    for similarity, json_correct in zip(similarities, json_accuracy_flags):
        y_true.append(1 if json_correct else 0)
        y_pred.append(1 if similarity >= similarity_threshold and json_correct else 0)

    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return precision, recall, f1
  \end{lstlisting}
    \caption{Classification Metrics}
    \label{lst:evalMetrics2}
\end{Listing}

\subsection{Challenges Encountered}

The classification metrics approach presented several challenges:
\begin{itemize}
    \item \textbf{Misleading FP Cases}: The approach could not effectively capture false positives because when the JSON was correct, the predictions were always marked as true positive, thus leading to an absence of FP cases.
    \item \textbf{Definition Misalignment}: The standard definitions of TP, FP, TN, and FN did not perfectly align with our use case, where both semantic similarity and JSON correctness were critical but evaluated differently than in binary classification tasks.
\end{itemize}

\section{Refined Approach}

\subsection{Adjustments to the Evaluation Method}

To address the issues with the initial approach, we refined our evaluation method to better capture the nuances of our use case:
\begin{itemize}
    \item \textbf{Combined Metric for Positive Predictions}: A positive prediction is now defined as having both a semantic similarity score above 0.65 and a correct JSON output.
    \item \textbf{Refined Definitions}: We redefined TP, FP, TN, and FN to better suit our chatbot's evaluation context.
\end{itemize}

\begin{Listing}
    \begin{lstlisting}[language=Python]
    def calculate_classification_metrics(similarities, json_accuracy_flags, similarity_threshold=0.8):
    y_true = []
    y_pred = []

    for similarity, json_correct in zip(similarities, json_accuracy_flags):
        # True label is positive if JSON is correct
        y_true.append(1 if json_correct else 0)

        # Predicted positive if similarity is above threshold and JSON is correct
        if similarity >= similarity_threshold and json_correct:
            y_pred.append(1)
        else:
            y_pred.append(0)

    # Calculate precision, recall, and F1 score
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return precision, recall, f1
  \end{lstlisting}
    \caption{Refined Classification Metrics}
    \label{lst:classificationRefined}
\end{Listing}

\subsection{Evaluation Results}

Using the refined approach, we obtained the following results:
\begin{itemize}
    \item \textbf{Precision}: [Value]
    \item \textbf{Recall}: [Value]
    \item \textbf{F1 Score}: [Value]
    \item \textbf{Semantic Similarity}: [Average Similarity]
    \item \textbf{JSON Accuracy}: [Accuracy Value]
\end{itemize}

These results indicate that the refined evaluation method provides a more accurate and reliable assessment of the chatbot's performance in handling user queries and controlling smart home devices.

\section{Conclusion}

In this chapter, we have discussed the evaluation metrics and methods used to assess the performance of our smart home chatbot. We highlighted the challenges faced with the initial classification metrics approach and presented a refined method that better aligns with the unique requirements of our use case. The evaluation results demonstrate the effectiveness of the chatbot in generating semantically accurate responses and correct JSON commands, ensuring a reliable and user-friendly smart home experience.

Here, you need to discuss about evaluation of your research result. You may wanna take a look at Goal Question Metric (GQM) paradigm that helps you in the process of evaluation \cite{caldiera1994goal}

\section{Study Design}
... what was the settings of your designed study to evaluate your results? e.g., You designed a Questionnaire to assess if your method increases the productivity of a programmer, explain and justify what population you chose, what was the questions/tasks and all necessary details. If you designed an experiment against a software system to collect measures and assess accuracy of your model, i.e., the contribution of your research, here explain e.g., how you collected measurements, what was characteristics of machines, etc.

\section{Results}
... what is the result of your e.g., Questionnaire or experimentation.. 
Presentation of Findings
Data Analysis
\subsection{User Experience Demonstration}
% include screenshots of example conversations 

\section{Discussion}
% not always response in correct language --> could be solved in telling the model in 
% which language to answer through the system language of the phone/app
... Based on the results argue about acceptance or rejection of your research hypothesis   .. 
Interpretation of Results
Comparison with Previous Studies
Limitations of the Study

\section{Threats to Validity}
... Discuss what threatens validity of your result. In case you could counteract them explain how. For experimentation in software engineering there is already a classification of this threats and a check-list \cite{DBLP:journals/ese/RunesonH09}.   
%LaTeX-Hinweise stehen in \cref{chap:latexhints}.

%noch etwas FÃ¼lltext
%\blinddocument
