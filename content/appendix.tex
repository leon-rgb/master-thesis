\begin{Listing}
    \begin{lstlisting}[language=Python]
def calculate_classification_metrics(similarities, json_accuracy_flags, similarity_threshold):
    y_true = []
    y_pred = []

    for similarity, json_correct in zip(similarities, json_accuracy_flags):
        y_true.append(1 if json_correct else 0)
        y_pred.append(1 if similarity >= similarity_threshold and json_correct else 0)

    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return precision, recall, f1
  \end{lstlisting}
    \caption{Classification Metrics}
    \label{lst:evalMetrics2}
\end{Listing}

\subsubsection{Initial Approach Using Classification Metrics}
Our first attempt at creating a combined metric to evaluate both semantic similarity and JSON accuracy involved adapting traditional classification metrics. This initial approach aimed to leverage the well-established framework of precision, recall, and F1 score to provide a comprehensive evaluation of our chatbot's performance.
\paragraph{Description of the Approach}
In this initial method, we defined the components of the classification metrics as follows:

True Positives (TP): Cases where the semantic similarity was above a threshold (e.g., 0.8) and the JSON was correct.
False Positives (FP): Cases where the semantic similarity was above the threshold but the JSON was incorrect.
True Negatives (TN): Cases where the semantic similarity was below the threshold and the JSON was incorrect.
False Negatives (FN): Cases where the semantic similarity was below the threshold but the JSON was correct.

The implementation of this approach is shown in \cref{lst:evalMetrics2}. This function, \texttt{calculate_classification_metrics}, took lists of similarity scores and JSON accuracy flags as input, along with a similarity threshold. It then computed precision, recall, and F1 score using the sklearn.metrics functions.
\paragraph{Challenges Encountered}
While this initial approach seemed promising, it presented several challenges and limitations:

\textbf{Misleading False Positive Cases}: One of the main issues with this approach was that it could not effectively capture false positives. When the JSON was correct, the predictions were always marked as true positive, regardless of the semantic similarity score. This led to an absence of false positive cases, which is problematic for calculating meaningful precision scores.
\textbf{Definition Misalignment}: The standard definitions of TP, FP, TN, and FN did not align perfectly with our use case. In our context, both semantic similarity and JSON correctness are critical, but they are evaluated differently than in typical binary classification tasks. This misalignment made the interpretation of the resulting metrics less intuitive and potentially misleading.
\textbf{Overemphasis on JSON Correctness}: The approach inadvertently placed more weight on JSON correctness than semantic similarity. A response with high semantic similarity but an incorrect JSON would be classified the same as a response with low semantic similarity and an incorrect JSON, which doesn't accurately reflect the nuanced performance we aimed to capture.
\textbf{Lack of Granularity}: The binary nature of classification metrics (correct or incorrect) didn't capture the continuous nature of semantic similarity scores. This resulted in a loss of nuanced information about the degree of similarity between the generated and expected responses.
\textbf{Threshold Sensitivity}: The performance metrics were highly sensitive to the chosen similarity threshold. Small changes in this threshold could lead to significant changes in the calculated metrics, making it difficult to choose a universally appropriate threshold.

These limitations highlighted the need for a more refined approach that could better represent the dual criteria of semantic similarity and JSON accuracy in our evaluation framework. This realization led to the development of our refined combined metric, which addresses these issues and provides a more accurate and meaningful evaluation of our chatbot's performance.